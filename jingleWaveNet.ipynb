{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "3w_tz6kJSsa8",
        "szdJZnVvSVIE",
        "Eq2yNcxP8hK2",
        "upOWt3d38WBX",
        "P1Ctaqx9WcsZ",
        "TjHzefw-ViIe",
        "m0QsGe7vBiGx",
        "cWEurJLOUqZB",
        "i4gOoYJVxYMx",
        "2XQRbHxRxotO",
        "-JfeIEvNxfIy",
        "ujLdslEIxu77"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "3w_tz6kJSsa8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCjgDxDe98VT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e967af9-e11e-4640-bcba-3b8095838d9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device count: 2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torchaudio\n",
        "import soundfile as sf\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "# DDP IMPORTS\n",
        "import torch.distributed as dist\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "# !pip install tinytag          #TODO: change tinytag to something with cross-platform support\n",
        "# from tinytag import TinyTag\n",
        "\n",
        "# HANNA AND ANNA ENV\n",
        "# drive.mount('/content/drive')\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# fma_small_path = 'drive/Shareddrives/Computer Audition Project/fma_small/'\n",
        "# from google.colab import drive\n",
        "\n",
        "\n",
        "# IZZY ENV\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") ## specify the GPU id's, GPU id's start from 0.\n",
        "fma_small_path = r\"C:\\Users\\ihargrav\\Desktop\\fma_small\"\n",
        "\n",
        "#SET UP CUDA ENVIRONMENT\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "print(f\"device count: {torch.cuda.device_count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoaders"
      ],
      "metadata": {
        "id": "szdJZnVvSVIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "class FMADataset(Dataset):\n",
        "  def __init__(self, directory, transform=None):\n",
        "    self.directory = directory\n",
        "    self.transform = transform\n",
        "    self.file_names = [f for f in os.listdir(directory) if f.endswith('.mp3') and not f.startswith(\".\")]\n",
        "    print(f\"File names: {self.file_names}\")\n",
        "\n",
        "  def __len__(self):\n",
        "    print(f\"dataloader len: {len(self.file_names)}\")\n",
        "    return len(self.file_names)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # print(\"entered get item\")\n",
        "    file_path = os.path.join(self.directory, self.file_names[idx])\n",
        "    audio, sr = librosa.load(file_path, sr=None)\n",
        "    # print(\"loaded file\")\n",
        "\n",
        "    # PREPROCESSING\n",
        "    librosa.util.normalize(audio) # normalize\n",
        "\n",
        "    # Resample\n",
        "    if sr != 16000:\n",
        "      audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
        "    # audio = np.pad(audio, (0, max(0, 16000*30 - len(audio))), mode='constant') # cut to 30s and pad with 0s\n",
        "    # if audio.size > 16000*30:\n",
        "    #   audio = audio[:(16000*30)]\n",
        "\n",
        "    audio_tensor = torch.tensor(audio, dtype=torch.float32) # cast to tensor\n",
        "    # audio_tensor = audio_tensor.unsqueeze(0)\n",
        "\n",
        "    # Companding transforms\n",
        "    audio_tensor = torch.div(audio_tensor, torch.max(torch.abs(audio_tensor))) # normalize\n",
        "    transform = torchaudio.transforms.MuLawEncoding(quantization_channels=256)\n",
        "    audio_tensor = transform(audio_tensor) # compand\n",
        "    audio_tensor = torch.nn.functional.one_hot(audio_tensor, num_classes=256).to(torch.float32)\n",
        "\n",
        "    return audio_tensor\n",
        "\n",
        "\n",
        "#Instantiate dataset\n",
        "dataset = FMADataset(fma_small_path)\n",
        "\n",
        "#Split for train, valid, and test\n",
        "train_size = int(len(dataset) * 0.8)\n",
        "valid_size = (len(dataset) - train_size) // 2\n",
        "test_size = len(dataset) - train_size - valid_size\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
        "\n",
        "#Instantiate dataloaders. Dimensions are {batch_size, length, channels}\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "print(f\"train size: {len(train_dataloader)}\")\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# debug\n",
        "# debug_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=False)\n"
      ],
      "metadata": {
        "id": "wLlP15wYTB3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WaveNet Building Blocks"
      ],
      "metadata": {
        "id": "Eq2yNcxP8hK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class myCausalConv1d(torch.nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    '''\n",
        "      in_channels   :   Number of features in the input signal\n",
        "                        ex: a color image -> 3 in_channels (R,G,B)\n",
        "                        ex: a black and white image -> 1 in_channel (black)\n",
        "\n",
        "      out_channels  :   Number of channels produced by the convolution\n",
        "    '''\n",
        "\n",
        "    super(myCausalConv1d, self).__init__()\n",
        "\n",
        "    # padding = (kernel_size - 1)*dilation + 1\n",
        "    # (1 extra padding to ensure L > 0)\n",
        "    # causal -> kernel_size = 1, dilation = 1\n",
        "    # therefor, causal -> padding = 1\n",
        "    self.conv = torch.nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1, padding=1, bias=False).to('cuda:0')\n",
        "\n",
        "    torch.nn.init.xavier_normal_(self.conv.weight, gain=1)  # init weights\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.conv(x.float())\n",
        "\n",
        "    # model doesn't use the current sample when predicting the current sample\n",
        "    return output[:,:,:-2]    # [N, C, L]\n",
        "\n",
        "\n",
        "class myDilatedConv1d(torch.nn.Module):\n",
        "  def __init__(self, channels, device, dilation=1):\n",
        "    super(myDilatedConv1d, self).__init__()\n",
        "    self.pad = dilation\n",
        "\n",
        "    # padding = (kernel_size - 1)*dilation\n",
        "    # kernel_size = 2 (always for wavenet)\n",
        "    # therefor, padding = (2-1)*dilation = dilation\n",
        "    self.conv = torch.nn.Conv1d(channels, channels, kernel_size=2, stride=1, dilation=dilation, padding=self.pad, bias=False).to(device)\n",
        "\n",
        "    torch.nn.init.xavier_normal_(self.conv.weight, gain=1)  # init weights\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.conv(x)\n",
        "\n",
        "    # causal (asymmetric padding)\n",
        "    return output[:,:,:-self.pad]   # [N, C, L]\n",
        "\n",
        "\n",
        "class myResidualBlock(torch.nn.Module):\n",
        "  def __init__(self, residual_channels, skip_channels, dilation, device):\n",
        "    super(myResidualBlock, self).__init__()\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "    self.dilated_conv = myDilatedConv1d(residual_channels, device, dilation=dilation).to(device)\n",
        "    self.residual_conv = torch.nn.Conv1d(residual_channels, residual_channels, kernel_size=1).to(device)\n",
        "    self.skip_conv = torch.nn.Conv1d(residual_channels, skip_channels, kernel_size=1).to(device)\n",
        "\n",
        "    self.gate_tanh = torch.nn.Tanh()\n",
        "    self.gate_sig = torch.nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x, skip_size):    # skip_size == the last output size (??)\n",
        "    # Dilate\n",
        "    dilated = self.dilated_conv(x)\n",
        "\n",
        "    # Gating\n",
        "    tanh_out = self.gate_tanh(dilated)\n",
        "    sig_out = self.gate_sig(dilated)\n",
        "    gated = tanh_out * sig_out\n",
        "\n",
        "    # Residual\n",
        "    output = self.residual_conv(gated)\n",
        "    input_cut = x[:, :, -output.size(2):]   # ensure same dimensions\n",
        "    output += input_cut\n",
        "\n",
        "    # Skip\n",
        "    skip = self.skip_conv(gated)\n",
        "    skip = skip[:, :, -skip_size:]  # ensure same dimensions\n",
        "\n",
        "    return output, skip   # [N, C, L]\n",
        "\n",
        "\n",
        "class myResidualStack(torch.nn.Module):\n",
        "  def __init__(self, layer_size, stack_size, residual_channels, skip_channels):\n",
        "    super(myResidualStack, self).__init__()\n",
        "\n",
        "    self.layer_size = layer_size    # 10 = layer[dilation=1, , 4, 8, 16, 32, 64, 128, 256, 512]\n",
        "    self.stack_size = stack_size    # 5 = stack[layer1, layer2, layer3, layer4, layer5]\n",
        "\n",
        "    self.residual_blocks = self.stack_blocks(residual_channels, skip_channels)\n",
        "\n",
        "\n",
        "  def stack_blocks(self, residual_chan, skip_chan):\n",
        "    residual_blocks = []\n",
        "    dilations = self.make_dilations()\n",
        "\n",
        "    devices = ['cuda:0', 'cuda:1']\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for d in dilations:\n",
        "      # print(f\"residual_chan: {residual_chan}, skip_chan: {skip_chan}, d: {d}\")  # debug\n",
        "      this_block = self.make_block(residual_chan, skip_chan, d, devices[count%2])\n",
        "      residual_blocks.append(this_block)\n",
        "\n",
        "      count += 1\n",
        "\n",
        "    return residual_blocks\n",
        "\n",
        "  def make_dilations(self):\n",
        "    dilations = []  # 1, 2, 4, 8, 16, ...\n",
        "\n",
        "    for s in range(self.stack_size):\n",
        "      for l in range(self.layer_size):\n",
        "        dilations.append(2 ** l)\n",
        "\n",
        "    return dilations\n",
        "\n",
        "  def make_block(self, residual_chan, skip_chan, dilation, device):\n",
        "    block = myResidualBlock(residual_chan, skip_chan, dilation, device)\n",
        "    return block\n",
        "\n",
        "\n",
        "  def forward(self, x, skip_size):\n",
        "    skip_connections = []\n",
        "    output = x\n",
        "\n",
        "    for block in self.residual_blocks:\n",
        "      output, skip = block(output.to(block.device), skip_size)  #TODO: make sure this doesn't kill everything\n",
        "      skip_connections.append(skip.to('cuda:0'))\n",
        "\n",
        "    return torch.stack(skip_connections)  # [K, N, C, L]\n",
        "\n",
        "\n",
        "class myOutConv(torch.nn.Module):\n",
        "  def __init__(self, skip_channels, out_channels):\n",
        "    super(myOutConv, self).__init__()\n",
        "\n",
        "    # 1x1 convolutions\n",
        "    self.conv1 = torch.nn.Conv1d(skip_channels, skip_channels, kernel_size=1)\n",
        "    self.conv2 = torch.nn.Conv1d(skip_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    self.relu = torch.nn.ReLU()\n",
        "    self.softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    torch.nn.init.xavier_normal_(self.conv1.weight, gain=1)  # init weights\n",
        "    self.conv1.bias.data.fill_(0)                            # init bias\n",
        "    torch.nn.init.xavier_normal_(self.conv2.weight, gain=1)  # init weights\n",
        "    self.conv2.bias.data.fill_(0)                            # init bias\n",
        "\n",
        "  def forward(self, x):\n",
        "    o = self.relu(x)\n",
        "    o = self.conv1(o)\n",
        "\n",
        "    o = self.relu(o)\n",
        "    o = self.conv2(o)\n",
        "\n",
        "    # DEBUG: TURNED OFF\n",
        "    # output = self.softmax(o)\n",
        "    output = o\n",
        "\n",
        "    return output   # [N, C, L]\n"
      ],
      "metadata": {
        "id": "nDH8tJ3j8ggo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WaveNet Model"
      ],
      "metadata": {
        "id": "upOWt3d38WBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class myWaveNet(torch.nn.Module):\n",
        "  def __init__(self, layer_size=8, stack_size=4, in_channels=256, residual_channels=32, skip_channels=32):\n",
        "    super(myWaveNet, self).__init__()\n",
        "\n",
        "    self.stack_size = stack_size\n",
        "    self.layer_size = layer_size\n",
        "    self.residual_channels = residual_channels\n",
        "    self.receptive_fields = np.sum([2 ** i for i in range(layer_size)] * self.stack_size)\n",
        "\n",
        "    self.first_conv = myCausalConv1d(in_channels, residual_channels).to('cuda:0')\n",
        "    self.residual_stack = myResidualStack(layer_size, stack_size, residual_channels, skip_channels)\n",
        "    self.final_conv = myOutConv(skip_channels, in_channels).to('cuda:0')\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.transpose(1, 2)   # [N, C, L]\n",
        "    size = int(x.size(2))\n",
        "    x = self.first_conv(x)  # [N, C, L]\n",
        "\n",
        "    #TODO\n",
        "    skip_connections = self.residual_stack(x, size)   # [K, N, C, L],  entire list on cuda:0\n",
        "    output = torch.sum(skip_connections, dim=0)   # [N, C, L]\n",
        "\n",
        "    output = self.final_conv(output)    # [N, C, L]\n",
        "    output = output.transpose(1, 2)  # [N, L, C]\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "lRv9KJvY-H2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "P1Ctaqx9WcsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building Model...\")\n",
        "model = myWaveNet()\n",
        "\n",
        "\n",
        "model_path = r\"C:\\Users\\ihargrav\\Desktop\\checkpoints\\savedModel_fmaSmall_best.pt\"\n",
        "checkpoint = torch.load(model_path)\n",
        "\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "print(\"Loading audio...\")\n",
        "audio_raw, sr = librosa.load(r\"C:\\Users\\ihargrav\\Desktop\\fma_small\\000002.mp3\")  # TODO: get actual mp3 path\n",
        "audio = librosa.util.normalize(audio_raw)\n",
        "\n",
        "# Resample\n",
        "if sr != 16000:\n",
        "  audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
        "audio = np.pad(audio, (0, max(0, 16000*30 - len(audio))), mode='constant') # cut to 30s and pad with 0s\n",
        "if audio.size > 16000*30:\n",
        "  audio = audio[:(16000*30)]\n",
        "\n",
        "audio_tensor = torch.tensor(audio, dtype=torch.float32) # cast to tensor\n",
        "audio_tensor = audio_tensor.unsqueeze(0)\n",
        "\n",
        "# Companding transforms\n",
        "audio_tensor = torch.div(audio_tensor, torch.max(torch.abs(audio_tensor))) # normalize again\n",
        "\n",
        "# compand\n",
        "transform = torchaudio.transforms.MuLawEncoding(quantization_channels=256)\n",
        "audio_tensor = transform(audio_tensor)\n",
        "\n",
        "# one hot\n",
        "audio_tensor = torch.nn.functional.one_hot(audio_tensor, num_classes=256).to(torch.float32)\n",
        "# print(f\"audio tensor: {audio_tensor.shape}\")\n",
        "audio_tensor = audio_tensor.to(device)\n",
        "\n",
        "target = torch.clone(audio_tensor)\n",
        "target = target.transpose(1, 2)\n",
        "\n",
        "minValLoss = np.inf\n",
        "loss_list = []\n",
        "loss_txt_path = r\"C:\\Users\\ihargrav\\Desktop\\checkpoints\\loss.txt\"\n",
        "model.train()\n",
        "\n",
        "output = model(audio_tensor)\n",
        "\n",
        "decode = torchaudio.transforms.MuLawDecoding(quantization_channels=256)\n",
        "\n",
        "# print(f\"sequence: {sequence.shape}\")\n",
        "sequence_ = torch.argmax(output, axis=2)\n",
        "# print(f\"sequence_: {sequence_.shape}\")\n",
        "sequence_ = sequence_.squeeze()\n",
        "print(f\"sequence_: {sequence_.shape}\")\n",
        "\n",
        "audio = decode(sequence_)\n",
        "\n",
        "audio = audio.to(\"cpu\")\n",
        "\n",
        "# print(audio.numpy())\n",
        "print(audio)\n",
        "\n",
        "sf.write(r\"C:\\Users\\ihargrav\\Desktop\\audio_gen\\test.wav\", audio, 16000)"
      ],
      "metadata": {
        "id": "Q9TITLs4N7-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation Model"
      ],
      "metadata": {
        "id": "TjHzefw-ViIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(torch.nn.Module):\n",
        "  def __init__(self, model, device, batch_size=1, channels=256):\n",
        "    super(Generator, self).__init__()\n",
        "\n",
        "    self.model = model\n",
        "    self.residual_stack = model.get_submodule(\"residual_stack\").residual_blocks\n",
        "    self.channels = channels\n",
        "    self.batch_size = batch_size\n",
        "    self.stack_size = model.stack_size\n",
        "    self.layer_size = model.layer_size\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "    self.in_conv = self.model.get_submodule(\"first_conv\").get_submodule(\"conv\")\n",
        "    self.out_conv = self.model.get_submodule(\"final_conv\")\n",
        "\n",
        "\n",
        "    self.queue_list = []    # {input, layer1, layer2, ...} == {[1], [2], [4], [8], ...}\n",
        "    self.w_r = []           # weights to filter residual (sample from the queue)\n",
        "    self.w_i = []           # weights to filter input to layer\n",
        "\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    # For every layer...\n",
        "    for b in range(self.stack_size):\n",
        "      for l in range(self.layer_size):\n",
        "        q_len = 2**l        # queue length == dilation size\n",
        "\n",
        "        # Initialize queue for this layer\n",
        "        self.queue_list.append(torch.zeros(q_len, batch_size, self.model.residual_channels).to(self.device)) # K-element list: [qL, N, C]\n",
        "\n",
        "        # Recall dilation weights for this layer\n",
        "        dil_conv = self.residual_stack[count].get_submodule(\"dilated_conv\").get_submodule(\"conv\")\n",
        "\n",
        "        w_d = dil_conv.get_parameter(\"weight\").to(self.device)  # [C, C, Kernel] == [32, 32, 2]\n",
        "\n",
        "        # print(f\"b: {b}, l: {l}, \\nw_d: {w_d}\")\n",
        "\n",
        "        self.w_r.append(w_d[:,:,0])\n",
        "        self.w_i.append(w_d[:,:,1])\n",
        "\n",
        "        count += 1\n",
        "\n",
        "\n",
        "  def push_back(self, queue, input):\n",
        "    # if    :   q = [1, 2, 3, 4]  and  in = 9\n",
        "    # then  :   [2, 3, 4, 9]\n",
        "\n",
        "    # queue :   [qL, N, C]\n",
        "    # input :   [N, C]\n",
        "\n",
        "    input = input.unsqueeze(0)\n",
        "    # print(queue)\n",
        "\n",
        "    queue = queue[1:, :, :]\n",
        "    queue = torch.cat((queue, input))\n",
        "\n",
        "    # print(f\"L: {len(queue)}\")\n",
        "\n",
        "    return queue\n",
        "\n",
        "  def causal_lin(self, input, res, count, activation = None):\n",
        "    # input  : [N, C, 1] *where C = 32*\n",
        "    # res    : [N, C] *where C = 32*\n",
        "    # b      : integer, number of blocks in model\n",
        "    # l      : integer, number of layers in each block\n",
        "    # output : [N, C, 1] *where C = 32*\n",
        "\n",
        "    # Get weights\n",
        "    wr = self.w_r[count]\n",
        "    wi = self.w_i[count]\n",
        "\n",
        "    # Apply weights\n",
        "    input = input.squeeze(2)\n",
        "    output = torch.matmul(input, wr) + torch.matmul(res, wi)\n",
        "\n",
        "    # print(f\"input: {input}\\nres: {res}\")\n",
        "\n",
        "    # Non-linear activation\n",
        "    if activation:\n",
        "      output = activation(output)\n",
        "\n",
        "    return output.unsqueeze(2)\n",
        "\n",
        "\n",
        "  def predict(self, input):\n",
        "    # input  :  [N, 256, 1]\n",
        "    # pred   :  [N, 32, 1]\n",
        "    # output :  [N, 256, 1]\n",
        "\n",
        "    input = self.in_conv(input)             # [N, 256, 1] -> [N, 32, 3]\n",
        "    input = input[:,:,1].unsqueeze(2)       # [N, 32, 3] -> [N, 32, 1]\n",
        "\n",
        "    # print(f\"\\npredict input: {input}\")\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    # Single pass through the network\n",
        "    for b in range(self.stack_size):\n",
        "      for l in range(self.layer_size):\n",
        "        residual_queue = self.queue_list[count]   # get this layer's queue\n",
        "        # print(f\"b: {b}, l: {l} \\nqueue: {residual_queue}\")\n",
        "\n",
        "        # print(f\"{count}, q: {residual_queue.shape}\")\n",
        "        pred = self.causal_lin(input, residual_queue[0], count, activation=None)   # calculate output of this layer\n",
        "\n",
        "        # print(f\"input: {input.squeeze(2)}\")\n",
        "\n",
        "        self.queue_list[count] = self.push_back(self.queue_list[count], input.squeeze(2))    # update this layer's queue\n",
        "\n",
        "        input = pred  # propogate result up to next layer\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    # print(f\"\\nprediction: {pred}\")\n",
        "    print(f\"\\nQUANT: {torch.argmax(pred, dim=1)}\")\n",
        "\n",
        "    output = pred\n",
        "    output = torch.nn.functional.relu(pred)\n",
        "    output = self.out_conv(output)    # [N, 32, 1] -> [N, 256, 1]\n",
        "    output = torch.nn.functional.softmax(output, dim=1)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "  def run(self, seq_len, input=None):\n",
        "    # input   : [N, C] where C = 256\n",
        "    # seq_len : integer, length of generated sequence\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    # Seed sample\n",
        "    if(input is None):\n",
        "      input = torch.zeros(self.batch_size, self.channels, 1)  # [N, 256, 1]\n",
        "      input[:, 128, :] = 1\n",
        "\n",
        "    input = input.to(self.device)\n",
        "\n",
        "    # Save first sample\n",
        "    print(f\"\\nBeginning sequence...\")\n",
        "    sample = self.predict(input)\n",
        "    predictions.append(sample)\n",
        "\n",
        "    # Generate sequence\n",
        "    print(f\"\\nGenerating sequence...\")\n",
        "    with torch.no_grad():\n",
        "      for s in range(seq_len - 1):\n",
        "        print(f\"\\rSample {s+2} / {seq_len}\", end='')\n",
        "\n",
        "        arg_max = torch.argmax(sample, dim=1)\n",
        "        input = torch.zeros(1, 256, 1).to('cuda:0') #FIXME :(((\n",
        "        input[:, arg_max, :] = 1\n",
        "\n",
        "        sample = self.predict(input)\n",
        "\n",
        "        # print(f\"sample shape: {sample.shape}\")\n",
        "\n",
        "        # print(f\"\\nsample: {torch.argmax(sample, dim=1)}\")\n",
        "\n",
        "        predictions.append(sample)\n",
        "\n",
        "        # print(torch.cuda.memory_summary(self.device))\n",
        "\n",
        "      print()\n",
        "\n",
        "      # print(f\"\\nPREDICTIONS: {predictions[1].shape}\")\n",
        "\n",
        "      sequence = torch.cat(predictions, dim=0)\n",
        "\n",
        "      return sequence.transpose(0,2)  # [N, C, L]\n",
        "\n",
        "def temperature_sampling(logits, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Adjusts the logits based on the temperature and samples from the distribution for each time step.\n",
        "    :param logits: Logits output from the model. Shape: [1, bit_depth, num_samples].\n",
        "    :param temperature: Temperature parameter to control randomness. Higher temperature\n",
        "                        increases randomness.\n",
        "    :return: Sampled indices based on the adjusted distribution. Shape: [num_samples].\n",
        "    \"\"\"\n",
        "    # Initialize an empty tensor to store the sampled indices\n",
        "    sampled_indices = torch.zeros(logits.shape[2], dtype=torch.long)\n",
        "\n",
        "    # Process each time step individually\n",
        "    for i in range(logits.shape[2]):\n",
        "        logits_sample = logits[:, :, i]  # Extract logits for the current sample\n",
        "\n",
        "        if temperature != 1.0:\n",
        "            # Adjust logits by the temperature\n",
        "            logits_sample = logits_sample / temperature\n",
        "\n",
        "        probabilities = torch.nn.functional.softmax(logits_sample, dim=1)\n",
        "        # Create a categorical distribution and sample from it\n",
        "        sample_dist = torch.distributions.Categorical(probs=probabilities)\n",
        "        sampled_indices[i] = sample_dist.sample()\n",
        "\n",
        "    return sampled_indices"
      ],
      "metadata": {
        "id": "cj6OxchcVoDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Song Experiment (training)"
      ],
      "metadata": {
        "id": "m0QsGe7vBiGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building Model...\")\n",
        "model = myWaveNet(stack_size=4, layer_size=8)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
        "\n",
        "print(\"Loading audio...\")\n",
        "audio_raw, sr = librosa.load(r\"C:\\Users\\ihargrav\\Desktop\\piano.mp3\")  # TODO: get actual mp3 path\n",
        "audio = librosa.util.normalize(audio_raw)\n",
        "\n",
        "# Resample\n",
        "if sr != 16000:\n",
        "  audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
        "audio = np.pad(audio, (0, max(0, 16000*30 - len(audio))), mode='constant') # cut to 30s and pad with 0s\n",
        "if audio.size > 16000*30:\n",
        "  audio = audio[:(16000*30)]\n",
        "\n",
        "audio_tensor = torch.tensor(audio, dtype=torch.float32) # cast to tensor\n",
        "audio_tensor = audio_tensor.unsqueeze(0)\n",
        "\n",
        "# Companding transforms\n",
        "audio_tensor = torch.div(audio_tensor, torch.max(torch.abs(audio_tensor))) # normalize again\n",
        "\n",
        "# compand\n",
        "transform = torchaudio.transforms.MuLawEncoding(quantization_channels=256)\n",
        "audio_tensor = transform(audio_tensor)\n",
        "\n",
        "# one hot\n",
        "audio_tensor = torch.nn.functional.one_hot(audio_tensor, num_classes=256).to(torch.float32)\n",
        "print(f\"audio tensor: {audio_tensor.shape}\")\n",
        "audio_tensor = audio_tensor.to('cuda:0')\n",
        "\n",
        "\n",
        "target = torch.clone(audio_tensor)\n",
        "target = target.transpose(1, 2)\n",
        "\n",
        "minValLoss = np.inf\n",
        "loss_list = []\n",
        "loss_txt_path = r\"C:\\Users\\ihargrav\\Desktop\\checkpoints\\loss.txt\"\n",
        "model.train()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10000):\n",
        "\n",
        "  print(f\"epoch: {epoch}\")\n",
        "\n",
        "  model.zero_grad()\n",
        "\n",
        "  output = model(audio_tensor)\n",
        "  print(\"finished forward pass\")\n",
        "  output = output.transpose(1,2)\n",
        "\n",
        "  loss = criterion(output, target)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  # scheduler.step()\n",
        "\n",
        "  print(f\"loss: {loss}\")\n",
        "\n",
        "  checkpoint = {\n",
        "      'state_dict': model.state_dict(),\n",
        "      'minValLoss': minValLoss\n",
        "  }\n",
        "  torch.save(checkpoint, r'C:\\Users\\ihargrav\\Desktop\\checkpoints\\full_model\\ioConv_last.pt')\n",
        "\n",
        "  count = 0\n",
        "  for block in model.get_submodule(\"residual_stack\").residual_blocks:\n",
        "    path = (f\"C:/Users/ihargrav/Desktop/checkpoints/full_model/b{count}_last.pt\")\n",
        "    torch.save(block.state_dict(), path)\n",
        "    count += 1\n",
        "\n",
        "  # TODO: write real validation method for whole dataset\n",
        "  if loss < minValLoss:\n",
        "    minValLoss = loss\n",
        "    checkpoint = {\n",
        "      'state_dict': model.state_dict(),\n",
        "      'minValLoss': minValLoss\n",
        "    }\n",
        "    torch.save(checkpoint, r'C:\\Users\\ihargrav\\Desktop\\checkpoints\\full_model\\ioConv_best.pt')\n",
        "\n",
        "    count = 0\n",
        "    for block in model.get_submodule(\"residual_stack\").residual_blocks:\n",
        "      path = (f\"C:/Users/ihargrav/Desktop/checkpoints/full_model/b{count}_best.pt\")\n",
        "      torch.save(block.state_dict(), path)\n",
        "      count += 1\n",
        "\n",
        "  loss_list.append(float(loss))\n",
        "  with open(loss_txt_path, 'w') as file:\n",
        "    for item in loss_list:\n",
        "      file.write(\"%s\\n\" % item)\n",
        "\n",
        "\n",
        "\n",
        "  # t.set_description(f\"epoch : {epoch}, loss {loss}\")\n",
        "\n",
        "print(\"\\nModel Finished\")\n",
        "print(f\"output: {output.shape}\")\n"
      ],
      "metadata": {
        "id": "kRb1d8XgBovx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Song Experiment (generation)"
      ],
      "metadata": {
        "id": "cWEurJLOUqZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model\n",
        "model_path = \"C:/Users/ihargrav/Desktop/checkpoints/full_model/ioConv_best.pt\"\n",
        "checkpoint = torch.load(model_path)\n",
        "\n",
        "model_ref = myWaveNet(stack_size=4, layer_size=8)\n",
        "\n",
        "# Load first and last convolutions\n",
        "model_ref.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "# Load residual blocks\n",
        "blocks_this = model_ref.get_submodule(\"residual_stack\").residual_blocks\n",
        "\n",
        "for idx in range(len(blocks_this)):\n",
        "  path = (f\"C:/Users/ihargrav/Desktop/checkpoints/full_model/b{idx}_best.pt\")\n",
        "  blocks_ref_dict = torch.load(path)\n",
        "  blocks_this[idx].load_state_dict(blocks_ref_dict)\n",
        "\n",
        "model_ref.eval()\n",
        "\n",
        "# Build Generator\n",
        "generator = Generator(model_ref, device='cuda:0').to('cuda:0')\n",
        "\n",
        "# Generate novel sequence\n",
        "sequence = generator.run(16000)  # prob distributions\n",
        "print(sequence.shape)\n",
        "\n",
        "# Apply temperature sampling to your sequence\n",
        "temperature = 1.0\n",
        "# sequence_ = temperature_sampling(sequence, temperature=temperature)\n",
        "\n",
        "sequence_ = torch.argmax(sequence, axis=1)\n",
        "sequence_ = sequence_.squeeze()\n",
        "\n",
        "# Convert to audio\n",
        "decode = torchaudio.transforms.MuLawDecoding(quantization_channels=256)\n",
        "audio = decode(sequence_)\n",
        "\n",
        "audio = audio.to(\"cpu\")\n",
        "\n",
        "# print(audio)\n",
        "\n",
        "sf.write(r\"C:\\Users\\ihargrav\\Desktop\\audio_gen\\test_generated.wav\", audio, 16000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "_bGCZYhmUvZS",
        "outputId": "3be271c0-e75f-4800-fc6e-96ca3f84cc40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m model_ref\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Build Generator\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mGenerator\u001b[49m(model_ref, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Generate novel sequence\u001b[39;00m\n\u001b[0;32m     24\u001b[0m sequence \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;241m16000\u001b[39m)  \u001b[38;5;66;03m# prob distributions\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'Generator' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## GET AUDIO DEBUG\n",
        "\n",
        "# Load Model\n",
        "model_path = \"C:/Users/ihargrav/Desktop/checkpoints/demo_singSong_longField/demo_singSong_longField_full_model/ioConv_best.pt\"\n",
        "checkpoint = torch.load(model_path)\n",
        "\n",
        "model_test = myWaveNet(stack_size=1, layer_size=24)\n",
        "model_test.load_state_dict(checkpoint['state_dict'])    # Load first and last convolutions!!!\n",
        "\n",
        "# Load residual blocks!!!\n",
        "blocks_this = model_test.get_submodule(\"residual_stack\").residual_blocks\n",
        "\n",
        "for idx in range(len(blocks_this)):\n",
        "  path = (f\"C:/Users/ihargrav/Desktop/checkpoints/demo_singSong_longField/demo_singSong_longField_full_model/b{idx}_best.pt\")\n",
        "  blocks_ref_dict = torch.load(path)\n",
        "  blocks_this[idx].load_state_dict(blocks_ref_dict)\n",
        "\n",
        "\n",
        "model_test.eval()\n",
        "\n",
        "\n",
        "# Load audio\n",
        "print(\"Loading audio...\")\n",
        "audio_raw, sr = librosa.load(r\"C:\\Users\\ihargrav\\Desktop\\fma_small\\000002.mp3\")\n",
        "audio = librosa.util.normalize(audio_raw)\n",
        "\n",
        "# Resample\n",
        "if sr != 16000:\n",
        "  audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
        "audio = np.pad(audio, (0, max(0, 16000*30 - len(audio))), mode='constant') # cut to 30s and pad with 0s\n",
        "if audio.size > 16000*30:\n",
        "  audio = audio[:(16000*30)]\n",
        "\n",
        "audio_tensor = torch.tensor(audio, dtype=torch.float32) # cast to tensor\n",
        "audio_tensor = audio_tensor.unsqueeze(0)\n",
        "\n",
        "# Companding transforms\n",
        "audio_tensor = torch.div(audio_tensor, torch.max(torch.abs(audio_tensor))) # normalize again\n",
        "\n",
        "# compand\n",
        "transform = torchaudio.transforms.MuLawEncoding(quantization_channels=256)\n",
        "audio_tensor = transform(audio_tensor)\n",
        "\n",
        "# one hot\n",
        "audio_tensor = torch.nn.functional.one_hot(audio_tensor, num_classes=256).to(torch.float32)\n",
        "audio_tensor = audio_tensor.to('cuda:0')\n",
        "\n",
        "output = model_test(audio_tensor)\n",
        "\n",
        "# Convert to audio\n",
        "decode = torchaudio.transforms.MuLawDecoding(quantization_channels=256)\n",
        "\n",
        "# print(f\"sequence: {sequence.shape}\")\n",
        "sequence_ = torch.argmax(output, axis=2)\n",
        "# print(f\"sequence_: {sequence_.shape}\")\n",
        "sequence_ = sequence_.squeeze()\n",
        "print(f\"sequence_: {sequence_.shape}\")\n",
        "\n",
        "audio = decode(sequence_)\n",
        "\n",
        "audio = audio.to(\"cpu\")\n",
        "print(audio)\n",
        "\n",
        "sf.write(r\"C:\\Users\\ihargrav\\Desktop\\audio_gen\\test_benchmark.wav\", audio, 16000)\n",
        "\n"
      ],
      "metadata": {
        "id": "306sUjy0W6zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FMA Training"
      ],
      "metadata": {
        "id": "gpoQygZyP-Ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = myWaveNet(stack_size=4, layer_size=8)\n",
        "\n",
        "# COMMENT OUT TO START FRESH, UNCOMMENT TO PICK UP ===========================\n",
        "model_path = \"C:/Users/ihargrav/Desktop/checkpoints/full_model/ioConv_best.pt\"\n",
        "checkpoint = torch.load(model_path)\n",
        "print('loaded model path')\n",
        "# Load first and last convolutions\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "# Load residual blocks\n",
        "blocks_this = model.get_submodule(\"residual_stack\").residual_blocks\n",
        "\n",
        "for idx in range(len(blocks_this)):\n",
        "  path = (f\"C:/Users/ihargrav/Desktop/checkpoints/full_model/b{idx}_best.pt\")\n",
        "  blocks_ref_dict = torch.load(path)\n",
        "  blocks_this[idx].load_state_dict(blocks_ref_dict)\n",
        "print('got residual blocks')\n",
        "# COMMENT OUT TO START FRESH, UNCOMMENT TO PICK UP ===========================\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "minValLoss = np.inf\n",
        "loss_list = []\n",
        "loss_txt_path = r\"C:\\Users\\ihargrav\\Desktop\\checkpoints\\loss_fmaSmall.txt\"\n",
        "model.train()\n",
        "\n",
        "## TRAINING LOOP\n",
        "\n",
        "for epoch in range(2000):\n",
        "  iterator = iter(train_dataloader)\n",
        "  with trange(len(train_dataloader)) as t:\n",
        "    for idx in t:\n",
        "      try:\n",
        "        # Load sample and target\n",
        "        sample = next(iterator)\n",
        "        sample = sample.to('cuda:0')\n",
        "        target = torch.clone(sample).transpose(1, 2)\n",
        "\n",
        "        # print(f\"epoch: {epoch}\")\n",
        "\n",
        "        # Forward pass\n",
        "        model.zero_grad()\n",
        "\n",
        "        output = model(sample)\n",
        "        # print(\"finished forward pass\")\n",
        "        output = output.transpose(1,2)\n",
        "\n",
        "        # Backprop\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print(f\"loss: {loss}\")\n",
        "\n",
        "        # Save model (last)\n",
        "        checkpoint = {\n",
        "          'state_dict': model.state_dict(),\n",
        "          'minValLoss': minValLoss\n",
        "        }\n",
        "        torch.save(checkpoint, 'C:/Users/ihargrav/Desktop/checkpoints/full_model/ioConv_last.pt')\n",
        "\n",
        "        count = 0\n",
        "        for block in model.get_submodule(\"residual_stack\").residual_blocks:\n",
        "          path = (f\"C:/Users/ihargrav/Desktop/checkpoints/full_model/b{count}_last.pt\")\n",
        "          torch.save(block.state_dict(), path)\n",
        "          count += 1\n",
        "\n",
        "        # save model (best)\n",
        "        if loss < minValLoss:\n",
        "          minValLoss = loss\n",
        "          checkpoint = {\n",
        "            'state_dict': model.state_dict(),\n",
        "            'minValLoss': minValLoss\n",
        "          }\n",
        "          torch.save(checkpoint, r'C:\\Users\\ihargrav\\Desktop\\checkpoints\\full_model\\ioConv_best.pt')\n",
        "\n",
        "          count = 0\n",
        "          for block in model.get_submodule(\"residual_stack\").residual_blocks:\n",
        "            path = (f\"C:/Users/ihargrav/Desktop/checkpoints/full_model/b{count}_best.pt\")\n",
        "            torch.save(block.state_dict(), path)\n",
        "            count += 1\n",
        "\n",
        "        # Update loss.txt\n",
        "        loss_list.append(float(loss))\n",
        "        with open(loss_txt_path, 'w') as file:\n",
        "          for item in loss_list:\n",
        "            file.write(\"%s\\n\" % item)\n",
        "\n",
        "        t.set_description(f\"epoch: {epoch}, loss: {loss}\")\n",
        "      except:\n",
        "        continue\n",
        "print(\"\\nModel Finished\")\n",
        "print(f\"output: {output.shape}\")"
      ],
      "metadata": {
        "id": "pZ0eiCrmR0F1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84aa72ff-028b-4292-80f1-f8c97d991455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded model path\n",
            "got residual blocks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 0, loss: 3.535236120223999:  25%|█████████▋                             | 1585/6400 [1:02:13<3:08:13,  2.35s/it]C:\\Users\\ihargrav\\AppData\\Local\\Temp\\ipykernel_18060\\1695237955.py:18: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=None)\n",
            "C:\\ProgramData\\anaconda3\\envs\\pyt\\lib\\site-packages\\librosa\\core\\audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "epoch: 0, loss: 3.3234918117523193: 100%|████████████████████████████████████████| 6400/6400 [4:08:47<00:00,  2.33s/it]\n",
            "epoch: 1, loss: 3.2723100185394287: 100%|████████████████████████████████████████| 6400/6400 [4:12:13<00:00,  2.36s/it]\n",
            "epoch: 2, loss: 4.3123884201049805: 100%|████████████████████████████████████████| 6400/6400 [4:10:33<00:00,  2.35s/it]\n",
            "epoch: 3, loss: 3.4535350799560547: 100%|████████████████████████████████████████| 6400/6400 [4:12:12<00:00,  2.36s/it]\n",
            "epoch: 4, loss: 4.760231018066406: 100%|█████████████████████████████████████████| 6400/6400 [4:13:45<00:00,  2.38s/it]\n",
            "epoch: 5, loss: 3.262321949005127: 100%|█████████████████████████████████████████| 6400/6400 [4:17:30<00:00,  2.41s/it]\n",
            "epoch: 6, loss: 3.8314266204833984: 100%|████████████████████████████████████████| 6400/6400 [4:14:15<00:00,  2.38s/it]\n",
            "epoch: 7, loss: 4.433996677398682: 100%|█████████████████████████████████████████| 6400/6400 [4:08:46<00:00,  2.33s/it]\n",
            "epoch: 8, loss: 3.3088788986206055: 100%|████████████████████████████████████████| 6400/6400 [4:09:13<00:00,  2.34s/it]\n",
            "epoch: 9, loss: 3.3920557498931885: 100%|████████████████████████████████████████| 6400/6400 [4:17:20<00:00,  2.41s/it]\n",
            "epoch: 10, loss: 2.9329910278320312: 100%|███████████████████████████████████████| 6400/6400 [4:20:01<00:00,  2.44s/it]\n",
            "epoch: 11, loss: 2.925213098526001: 100%|████████████████████████████████████████| 6400/6400 [4:39:59<00:00,  2.62s/it]\n",
            "epoch: 12, loss: 3.5848963260650635: 100%|███████████████████████████████████████| 6400/6400 [4:16:53<00:00,  2.41s/it]\n",
            "epoch: 13, loss: 3.5233395099639893: 100%|███████████████████████████████████████| 6400/6400 [4:14:40<00:00,  2.39s/it]\n",
            "epoch: 14, loss: 3.7124102115631104: 100%|███████████████████████████████████████| 6400/6400 [4:15:05<00:00,  2.39s/it]\n",
            "epoch: 15, loss: 3.851644515991211: 100%|████████████████████████████████████████| 6400/6400 [4:15:58<00:00,  2.40s/it]\n",
            "epoch: 16, loss: 3.5475149154663086: 100%|███████████████████████████████████████| 6400/6400 [4:17:20<00:00,  2.41s/it]\n",
            "epoch: 17, loss: 3.35915470123291:  42%|████████████████▏                      | 2660/6400 [1:47:18<2:31:12,  2.43s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FMA Validation"
      ],
      "metadata": {
        "id": "-jqPyPw8kQb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model\n",
        "model_path = \"C:/Users/ihargrav/Desktop/checkpoints/full_model/ioConv_best.pt\"\n",
        "checkpoint = torch.load(model_path)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "model = myWaveNet(stack_size=4, layer_size=8)\n",
        "\n",
        "# Load first and last convolutions\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "# Load residual blocks\n",
        "blocks_this = model.get_submodule(\"residual_stack\").residual_blocks\n",
        "\n",
        "for idx in range(len(blocks_this)):\n",
        "  path = (f\"C:/Users/ihargrav/Desktop/checkpoints/full_model/b{idx}_best.pt\")\n",
        "  blocks_ref_dict = torch.load(path)\n",
        "  blocks_this[idx].load_state_dict(blocks_ref_dict)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "## VALIDATION LOOP\n",
        "\n",
        "val_loss_list = []\n",
        "val_loss_txt_path = r\"C:\\Users\\ihargrav\\Desktop\\checkpoints\\val_loss_fmaSmall.txt\"\n",
        "\n",
        "val_loss = 0\n",
        "model.eval()\n",
        "model.zero_grad()\n",
        "\n",
        "for num, sample in enumerate(valid_dataloader):\n",
        "  # Load sample and target\n",
        "\n",
        "  sample = sample.to('cuda:0')\n",
        "  target = torch.clone(sample).transpose(1, 2)\n",
        "\n",
        "  # Forward pass\n",
        "  model.zero_grad()\n",
        "\n",
        "  output = model(sample)\n",
        "  # print(\"finished forward pass\")\n",
        "  output = output.transpose(1,2)\n",
        "\n",
        "  # Update loss\n",
        "  val_loss += criterion(output, target).item()\n",
        "\n",
        "  print(f\"\\r\\nCumulative loss: {val_loss}\", end='')\n",
        "\n",
        "  val_loss_list.append(float(val_loss))\n",
        "  with open(val_loss_txt_path, 'w') as file:\n",
        "    for item in val_loss_list:\n",
        "      file.write(\"%s\\n\" % item)\n",
        "\n",
        "print(f\"Loss: {val_loss / len(valid_dataloader)}\")\n",
        "\n",
        "# Save model (best)\n",
        "if val_loss < minValLoss:\n",
        "  minValLoss = val_loss\n",
        "  checkpoint = {\n",
        "    'state_dict': model.state_dict(),\n",
        "    'minValLoss': minValLoss\n",
        "  }\n",
        "  torch.save(checkpoint, r'C:\\Users\\ihargrav\\Desktop\\checkpoints\\full_model\\ioConv_best.pt')\n",
        "\n",
        "  count = 0\n",
        "  for block in model.get_submodule(\"residual_stack\").residual_blocks:\n",
        "    path = (f\"C:/Users/ihargrav/Desktop/checkpoints/full_model/b{count}_best.pt\")\n",
        "    torch.save(block.state_dict(), path)\n",
        "    count += 1"
      ],
      "metadata": {
        "id": "8PkxECeumdOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jingle Dataloader"
      ],
      "metadata": {
        "id": "i4gOoYJVxYMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "class JingleDataset(Dataset):\n",
        "  def __init__(self, directory, transform=None):\n",
        "    self.directory = directory\n",
        "    self.transform = transform\n",
        "    self.file_names = [f for f in os.listdir(directory) if f.endswith('.mp3') and not f.startswith(\".\")]\n",
        "    print(f\"File names: {self.file_names}\")\n",
        "\n",
        "\n",
        "    ######### NEW ###########\n",
        "    # Create a mapping from mood names to numerical identifiers\n",
        "    self.mood_enum = {'upb': 0, 'rlx': 1, 'plf': 2, 'mys': 3, 'mel': 4}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.file_names)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    file_path = os.path.join(self.directory, self.file_names[idx])\n",
        "    #audio, sr = librosa.load(file_path, sr=None)\n",
        "    audio, sr = sf.read(file_path)\n",
        "    print(audio.shape)\n",
        "    audio = audio[:,1]\n",
        "    print(audio.shape)\n",
        "\n",
        "    print(f\"PATH: {file_path}\\nAUDIO: {audio.shape}\")\n",
        "\n",
        "    # print(\"loaded file\")\n",
        "    # audio_metadata = TinyTag.get(file_path)\n",
        "\n",
        "   ######### NEW ###########\n",
        "    # GET THE MOOD\n",
        "    name = self.file_names[idx]\n",
        "    mood_name = name[:3]\n",
        "    mood_tag = self.mood_enum.get(mood_name, -1)  # Use -1 for unknown moods\n",
        "\n",
        "    print(f\"file name: {self.file_names[idx]}\")\n",
        "    print(f\"name = {mood_name}, tag = {mood_tag}\")\n",
        "\n",
        "    # PREPROCESSING\n",
        "    librosa.util.normalize(audio) # normalize\n",
        "\n",
        "    # Resample\n",
        "    if sr != 16000:\n",
        "      audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
        "    audio = np.pad(audio, (0, max(0, 16000*30 - len(audio))), mode='constant') # cut to 30s and pad with 0s\n",
        "    if audio.size > 16000*30:\n",
        "      audio = audio[:(16000*30)]\n",
        "\n",
        "    audio_tensor = torch.tensor(audio, dtype=torch.float32) # cast to tensor\n",
        "\n",
        "    # Companding transforms\n",
        "    audio_tensor = torch.div(audio_tensor, torch.max(torch.abs(audio_tensor))) # normalize\n",
        "    transform = torchaudio.transforms.MuLawEncoding(quantization_channels=256)\n",
        "    audio_tensor = transform(audio_tensor) # compand\n",
        "    #print(f\"after companding transform: {audio_tensor.shape}\")\n",
        "    audio_tensor = torch.nn.functional.one_hot(audio_tensor, num_classes=256).to(torch.float32)\n",
        "\n",
        "\n",
        "\n",
        "    return audio_tensor, mood_tag # , audio_metadata.genre\n",
        "\n",
        "######### NEW ###########\n",
        "#Instantiate dataset\n",
        "dataset = JingleDataset(jingle_path)\n",
        "\n",
        "# names = dataset.file_names;\n",
        "# for n in names :\n",
        "#\n",
        "#    mood_name = n[:3]\n",
        "#    mood_tag = dataset.mood_enum.get(mood_name, -1)  # Use -1 for unknown moods\n",
        "#    print(f\"filename = {n}, name = {mood_name}, tag = {mood_tag}\")\n",
        "\n",
        "#Split for train, valid, and test\n",
        "train_size = int(len(dataset) * 0.8)\n",
        "valid_size = (len(dataset) - train_size) // 2\n",
        "test_size = len(dataset) - train_size - valid_size\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
        "\n",
        "#Instantiate dataloaders. Dimensions are {1, batch_size, sample_num}\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# debug\n",
        "debug_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "print(len(debug_dataloader.dataset))\n",
        "print(len(debug_dataloader))\n",
        "\n",
        "# for idx in range(len(train_dataset)):\n",
        "print(train_dataset)\n",
        "#for batch in debug_dataloader:\n",
        "\n",
        "\n",
        "\n",
        "    # print(f\"Sample {idx}: {sample}, Tag: {tag}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5dHg1BBbxWrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jingle Building Blocks"
      ],
      "metadata": {
        "id": "2XQRbHxRxotO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class myCausalConv1d(torch.nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    '''\n",
        "      in_channels   :   Number of features in the input signal\n",
        "                        ex: a color image -> 3 in_channels (R,G,B)\n",
        "                        ex: a black and white image -> 1 in_channel (black)\n",
        "\n",
        "      out_channels  :   Number of channels produced by the convolution\n",
        "    '''\n",
        "\n",
        "    super(myCausalConv1d, self).__init__()\n",
        "\n",
        "    # padding = (kernel_size - 1)*dilation + 1\n",
        "    # (1 extra padding to ensure L > 0)\n",
        "    # causal -> kernel_size = 1, dilation = 1\n",
        "    # therefor, causal -> padding = 1\n",
        "    self.conv = torch.nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1, padding=1, bias=False).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.conv(x.float())\n",
        "\n",
        "    # model doesn't use the current sample when predicting the current sample\n",
        "    return output[:,:,:-2]    # [N, C, L]\n",
        "\n",
        "\n",
        "class myDilatedConv1d(torch.nn.Module):\n",
        "  def __init__(self, channels, dilation=1):\n",
        "    super(myDilatedConv1d, self).__init__()\n",
        "    self.pad = dilation\n",
        "\n",
        "    # padding = (kernel_size - 1)*dilation\n",
        "    # kernel_size = 2 (always for wavenet)\n",
        "    # therefor, padding = (2-1)*dilation = dilation\n",
        "    self.conv = torch.nn.Conv1d(channels, channels, kernel_size=2, stride=1, dilation=dilation, padding=self.pad, bias=False).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.conv(x)\n",
        "\n",
        "    # causal (asymmetric padding)\n",
        "    return output[:,:,:-self.pad]   # [N, C, L]\n",
        "\n",
        "\n",
        "class myResidualBlock(torch.nn.Module):\n",
        "  def __init__(self, residual_channels, skip_channels, dilation):\n",
        "    super(myResidualBlock, self).__init__()\n",
        "\n",
        "    self.dilated_conv = myDilatedConv1d(residual_channels, dilation=dilation).to(device)\n",
        "    self.residual_conv = torch.nn.Conv1d(residual_channels, residual_channels, kernel_size=1).to(device)\n",
        "    self.skip_conv = torch.nn.Conv1d(residual_channels, skip_channels, kernel_size=1).to(device)\n",
        "\n",
        "    self.gate_tanh = torch.nn.Tanh()\n",
        "    self.gate_sig = torch.nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x, skip_size):    # skip_size == the last output size (??)\n",
        "    # Dilate\n",
        "    dilated = self.dilated_conv(x)\n",
        "\n",
        "    # Gating\n",
        "    tanh_out = self.gate_tanh(dilated)\n",
        "    sig_out = self.gate_sig(dilated)\n",
        "    gated = tanh_out * sig_out\n",
        "\n",
        "    # Residual\n",
        "    output = self.residual_conv(gated)\n",
        "    input_cut = x[:, :, -output.size(2):]   # ensure same dimensions\n",
        "    output += input_cut\n",
        "\n",
        "    # Skip\n",
        "    skip = self.skip_conv(gated)\n",
        "    skip = skip[:, :, -skip_size:]  # ensure same dimensions\n",
        "\n",
        "    return output, skip   # [N, C, L]\n",
        "\n",
        "\n",
        "class myResidualStack(torch.nn.Module):\n",
        "  def __init__(self, layer_size, stack_size, residual_channels, skip_channels):\n",
        "    super(myResidualStack, self).__init__()\n",
        "\n",
        "    self.layer_size = layer_size    # 10 = layer[dilation=1, , 4, 8, 16, 32, 64, 128, 256, 512]\n",
        "    self.stack_size = stack_size    # 5 = stack[layer1, layer2, layer3, layer4, layer5]\n",
        "\n",
        "    self.residual_blocks = self.stack_blocks(residual_channels, skip_channels)\n",
        "\n",
        "\n",
        "  def stack_blocks(self, residual_chan, skip_chan):\n",
        "    residual_blocks = []\n",
        "    dilations = self.make_dilations()\n",
        "\n",
        "    for d in dilations:\n",
        "      # print(f\"residual_chan: {residual_chan}, skip_chan: {skip_chan}, d: {d}\")  # debug\n",
        "      this_block = self.make_block(residual_chan, skip_chan, d)\n",
        "      residual_blocks.append(this_block)\n",
        "\n",
        "    return residual_blocks\n",
        "\n",
        "  def make_dilations(self):\n",
        "    dilations = []  # 1, 2, 4, 8, 16, ...\n",
        "\n",
        "    for s in range(self.stack_size):\n",
        "      for l in range(self.layer_size):\n",
        "        dilations.append(2 ** l)\n",
        "\n",
        "    return dilations\n",
        "\n",
        "  def make_block(self, residual_chan, skip_chan, dilation):\n",
        "    block = myResidualBlock(residual_chan, skip_chan, dilation)\n",
        "    return block\n",
        "\n",
        "\n",
        "  def forward(self, x, skip_size):\n",
        "    skip_connections = []\n",
        "    output = x\n",
        "\n",
        "    for block in self.residual_blocks:\n",
        "      output, skip = block(output, skip_size)\n",
        "      skip_connections.append(skip)\n",
        "\n",
        "    return torch.stack(skip_connections)  # [K, N, C, L]\n",
        "\n",
        "\n",
        "class myOutConv(torch.nn.Module):\n",
        "  def __init__(self, skip_channels, out_channels):\n",
        "    super(myOutConv, self).__init__()\n",
        "\n",
        "    # 1x1 convolutions\n",
        "    self.conv1 = torch.nn.Conv1d(skip_channels, skip_channels, kernel_size=1).to(device)\n",
        "    self.conv2 = torch.nn.Conv1d(skip_channels, out_channels, kernel_size=1).to(device)\n",
        "\n",
        "    self.relu = torch.nn.ReLU()\n",
        "    self.softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    o = self.relu(x)\n",
        "    o = self.conv1(o)\n",
        "\n",
        "    o = self.relu(o)\n",
        "    o = self.conv2(o)\n",
        "\n",
        "    output = self.softmax(o)\n",
        "\n",
        "    return output   # [N, C, L]\n"
      ],
      "metadata": {
        "id": "kEIX9cwHxt8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jingle Model"
      ],
      "metadata": {
        "id": "-JfeIEvNxfIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class myWaveNet(torch.nn.Module):\n",
        "  ######### NEW ########### -- added gc channels and gc cardinality as arguments\n",
        "  def __init__(self, layer_size=10, stack_size=4, in_channels=256, residual_channels=32, skip_channels=32, global_condition_channels=None,global_condition_cardinality=None):\n",
        "    super(myWaveNet, self).__init__()\n",
        "\n",
        "    self.receptive_fields = np.sum([2 ** i for i in range(layer_size)] * stack_size)\n",
        "\n",
        "    self.first_conv = myCausalConv1d(in_channels, residual_channels)\n",
        "    self.residual_stack = myResidualStack(layer_size, stack_size, residual_channels, skip_channels)\n",
        "    self.final_conv = myOutConv(skip_channels, in_channels)\n",
        "\n",
        "    ######### NEW ###########\n",
        "    # Embedding layer for global conditioning\n",
        "    if global_condition_channels is not None and global_condition_cardinality is not None:\n",
        "        self.embedding = torch.nn.Embedding(global_condition_cardinality, global_condition_channels)\n",
        "    else:\n",
        "        self.embedding = None\n",
        "    # set gc channels and gc cardinality\n",
        "    #self.global_condition_channels = global_condition_channels\n",
        "    #self.global_condition_cardinality = global_condition_cardinality\n",
        "\n",
        "  def forward(self, x, mood_tag=None):\n",
        "\n",
        "    # print(\"\\nInitial Transopose...\")\n",
        "    # print(f\"in: {x.shape}\")\n",
        "    x = x.transpose(1, 2)   # [N, C, L]\n",
        "    # print(f\"out: {x.shape}\")\n",
        "\n",
        "    # Embed global condition if available\n",
        "    if self.embedding is not None and mood_tag is not None:\n",
        "      gc_embedding = self.embedding(mood_tag)\n",
        "      print(f\"x shape: {x.shape}\")\n",
        "      print(f\"gc_embedding shape: {gc_embedding.shape}\")\n",
        "\n",
        "      gc_embedding = gc_embedding.unsqueeze(2).expand(-1, x.size(1), -1)\n",
        "\n",
        "      print(f\"gc_embedding shape after expansion: {gc_embedding.shape}\")\n",
        "      x = torch.cat([x,gc_embedding], dim=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    size = int(x.size(2))\n",
        "    # print(f\"size: {size}\")\n",
        "\n",
        "    # print(\"\\nFirst Conv...\")\n",
        "    # print(f\"in: {x.shape}\")\n",
        "    x = self.first_conv(x)  # [N, C, L]\n",
        "    # print(f\"out: {x.shape}\")\n",
        "\n",
        "    # Apply residual stack\n",
        "    # print(\"\\nResidual Stack...\")\n",
        "    # print(f\"in: {x.shape}\")\n",
        "    skip_connections = self.residual_stack(x, size)   # [K, N, C, L]\n",
        "    # print(f\"out: {skip_connections.shape}\")\n",
        "\n",
        "    # print(\"\\nSumming Residual Stack...\")\n",
        "    # print(f\"in: {skip_connections.shape}\")\n",
        "    output = torch.sum(skip_connections, dim=0)   # [N, C, L]\n",
        "    # print(f\"out: {output.shape}\")\n",
        "\n",
        "    # print(\"\\nFinal Conv...\")\n",
        "    # print(f\"in: {output.shape}\")\n",
        "    output = self.final_conv(output)    # [N, C, L]\n",
        "    # print(f\"out: {output.shape}\")\n",
        "\n",
        "    # print(\"\\nFinal Transpose...\")\n",
        "    # print(f\"in: {output.shape}\")\n",
        "    output = output.transpose(1, 2)  # [N, L, C]\n",
        "    # print(f\"out: {output.shape}\")\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "hBHB9vGcxl3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jingle Debug Training"
      ],
      "metadata": {
        "id": "ujLdslEIxu77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building Model...\")\n",
        "model = myWaveNet(global_condition_channels=1,global_condition_cardinality=5)\n",
        "model.to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "#for num, (sample, tag) in enumerate(debug_dataloader):\n",
        "#  print(num)\n",
        "#  print(f\"sample={sample}, tag={tag}\")\n",
        "\n",
        "minValLoss = np.inf\n",
        "loss_list = []\n",
        "loss_txt_path = \"loss.txt\"\n",
        "model.train()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2):\n",
        "  print(f\"epoch: {epoch + 1}\")\n",
        "  for num, (sample, tag) in enumerate(debug_dataloader):\n",
        "    print(f\"SAMPLE: {sample}\")\n",
        "    sample = sample.to(device)\n",
        "    tag = tag.to(device)\n",
        "    target = torch.clone(sample).transpose(1, 2).to(device)\n",
        "\n",
        "    print(f\"epoch: {epoch+1}, batch number {num} of {len(debug_dataloader)}\")\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    print(f\"Audio Shape: {sample.shape}\")\n",
        "    print(f\"Audio: {sample}\")\n",
        "    print(f\"Tag Shape: {tag.shape}\")\n",
        "    print(f\"Tag: {tag}\")\n",
        "\n",
        "    output = model(sample, mood_tag = tag)\n",
        "    print(\"finished forward pass\")\n",
        "    output = output.transpose(1,2)\n",
        "\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"loss: {loss}\")\n",
        "\n",
        "    loss_list.append(float(loss))\n",
        "    with open(loss_txt_path, 'w') as file:\n",
        "      for item in loss_list:\n",
        "        file.write(\"%s\\n\" % item)\n",
        "\n",
        "\n",
        "  checkpoint = {\n",
        "    'state_dict': model.state_dict(),\n",
        "    'minValLoss': minValLoss\n",
        "  }\n",
        "  torch.save(checkpoint, 'savedModel_jingleConditioning_last.pt')\n",
        "\n",
        "  # TODO: write real validation method for whole dataset\n",
        "  if loss < minValLoss:\n",
        "    minValLoss = loss\n",
        "    checkpoint = {\n",
        "      'state_dict': model.state_dict(),\n",
        "      'minValLoss': minValLoss\n",
        "    }\n",
        "    torch.save(checkpoint, 'savedModel_jingleConditioning_best.pt')\n",
        "\n",
        "\n",
        "  # t.set_description(f\"epoch : {epoch}, loss {loss}\")\n",
        "\n",
        "print(\"\\nModel Finished\")\n",
        "print(f\"output: {output.shape}\")"
      ],
      "metadata": {
        "id": "By2l1ae7xxb7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}